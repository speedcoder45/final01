{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31239,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# =========================\n# KNN baseline + Viz + tuning + metrics + submission\n# =========================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    roc_auc_score, log_loss, confusion_matrix\n)\n\n# -------------------------\n# 1) LOAD\n# -------------------------\ntrain = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\ntest  = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape :\", test.shape)\n\n# -------------------------\n# 2) Quick Data Visualisation (light)\n# -------------------------\n# target distribution\nplt.figure(figsize=(6,3))\nsns.countplot(x=train['Status'])\nplt.title(\"Target distribution\")\nplt.show()\n\n# numeric overview & small correlation heatmap\nnum_cols = train.select_dtypes(include=['int64','float64']).columns.tolist()\nif len(num_cols) > 0:\n    plt.figure(figsize=(8,6))\n    sns.heatmap(train[num_cols].corr(), cmap=\"coolwarm\", annot=False)\n    plt.title(\"Numeric feature correlation (small)\")\n    plt.show()\n\n    # show hist of first up to 4 numeric columns (keeps it light)\n    for col in num_cols[:4]:\n        plt.figure(figsize=(6,2.5))\n        sns.histplot(train[col].dropna(), kde=True)\n        plt.title(f\"Hist: {col}\")\n        plt.show()\n\n# simple outlier-check (boxplots for first 4 numeric cols)\nfor col in num_cols[:4]:\n    plt.figure(figsize=(6,2.5))\n    sns.boxplot(x=train[col].dropna())\n    plt.title(f\"Outlier check (visual): {col}\")\n    plt.show()\n\n# -------------------------\n# 3) MISSING VALUES (same safe strategy, impute)\n# -------------------------\n# categorical columns except target\ncat_cols = train.select_dtypes(include=['object']).columns.tolist()\nif 'Status' in cat_cols:\n    cat_cols.remove('Status')\n\n# numeric columns recalculated\nnum_cols = train.select_dtypes(include=['int64','float64']).columns.tolist()\n\n# fill categorical with mode, numeric with mean (train→test safe)\nfor c in cat_cols:\n    train[c] = train[c].fillna(train[c].mode()[0])\n    if c in test.columns:\n        test[c]  = test[c].fillna(test[c].mode()[0])\n\nfor c in num_cols:\n    train[c] = train[c].fillna(train[c].mean())\n    if c in test.columns:\n        test[c]  = test[c].fillna(test[c].mean())\n\n# -------------------------\n# 4) ENCODING target + safe label-encoding for categorical features\n# -------------------------\ntarget_le = LabelEncoder()\ny = target_le.fit_transform(train['Status'])              # numeric labels for metrics\n\n# For categorical features: safe label-encode using train+test values (keeps mapping consistent)\nfrom sklearn.preprocessing import OrdinalEncoder\n# We will use OrdinalEncoder to transform categorical columns to integer codes for KNN,\n# but we must ensure unseen categories in test are handled — convert to string and fit on concat.\nfor c in cat_cols:\n    # convert to string to avoid issues with mixed types\n    combined = pd.concat([train[c].astype(str), test[c].astype(str)], axis=0)\n    enc = LabelEncoder()\n    enc.fit(combined)\n    train[c] = enc.transform(train[c].astype(str))\n    test[c]  = enc.transform(test[c].astype(str))\n\n# -------------------------\n# 5) FEATURES & SPLIT\n# -------------------------\nX = train.drop(columns=['Status']).copy()\n# ensure test_features match X columns (drop id if present)\ntest_features = test.copy()\nif 'id' in X.columns:\n    X = X.drop(columns=['id'])\nif 'id' in test_features.columns:\n    test_features = test_features.drop(columns=['id'])\n\n# train/val split (use encoded y)\nX_train, X_val, y_train, y_val = train_test_split(\n    X, y, test_size=0.20, random_state=42, stratify=y\n)\n\n# -------------------------\n# 6) FEATURE SCALING (important for KNN)\n# -------------------------\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_val_scaled   = scaler.transform(X_val)\n# scale test features using same scaler (align columns)\ntest_scaled = scaler.transform(test_features[X.columns])\n\n# -------------------------\n# 7) BASE KNN model + SMALL hyperparameter tuning (light & safe)\n#    We tune for NEG_LOG_LOSS to improve probability quality.\n# -------------------------\nknn = KNeighborsClassifier()\n\nparam_dist = {\n    \"n_neighbors\": [3,5,7,10,15,20,30],\n    \"weights\": [\"uniform\",\"distance\"],\n    \"p\": [1,2]   # 1 = manhattan, 2 = euclidean\n}\n\nrs = RandomizedSearchCV(\n    knn,\n    param_distributions=param_dist,\n    n_iter=8,        # keep small so it finishes\n    cv=3,\n    scoring='neg_log_loss',   # tune probabilities\n    n_jobs=-1,\n    random_state=42,\n    verbose=0\n)\n\nrs.fit(X_train_scaled, y_train)\nbest_knn = rs.best_estimator_\nprint(\"Best KNN params:\", rs.best_params_)\n\n# -------------------------\n# 8) EVALUATION on validation set (all requested metrics)\n# -------------------------\ny_val_pred = best_knn.predict(X_val_scaled)\ny_val_proba = best_knn.predict_proba(X_val_scaled)\n\nacc = accuracy_score(y_val, y_val_pred)\nprec = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)\nrec  = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)\nf1   = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)\n# roc_auc for multiclass requires probability estimates\nroc_auc = roc_auc_score(y_val, y_val_proba, multi_class='ovr')\nll = log_loss(y_val, y_val_proba)\n\nprint(\"\\nValidation metrics (KNN tuned):\")\nprint(\"Accuracy :\", acc)\nprint(\"Precision:\", prec)\nprint(\"Recall   :\", rec)\nprint(\"F1 Score :\", f1)\nprint(\"ROC-AUC  :\", roc_auc)\nprint(\"Log Loss :\", ll)\n\n# confusion matrix (visual)\ncm = confusion_matrix(y_val, y_val_pred)\nplt.figure(figsize=(5,4))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.title(\"Confusion matrix (validation)\")\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.show()\n\n# -------------------------\n# 9) FINAL PREDICTIONS for submission (probabilities)\n# -------------------------\nfinal_proba = best_knn.predict_proba(test_scaled)\n\n# columns should be Status_{class} in the same order as target_le.classes_\ncols = [f\"Status_{cls}\" for cls in target_le.classes_]\n\nsubmission = pd.DataFrame(final_proba, columns=cols)\n# id column from original test\nif 'id' in test.columns:\n    submission.insert(0, 'id', test['id'])\nelse:\n    submission.insert(0, 'id', np.arange(len(test)))\n\nsubmission.to_csv(\"/kaggle/working/Answer_with_metrics.csv\", index=False)\nprint(\"\\nSaved submission to /kaggle/working/Answer_with_metrics.csv\")\nsubmission.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}